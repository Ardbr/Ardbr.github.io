<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>diffusion Ch01 | creat's HomePage</title><meta name="author" content="creat"><meta name="copyright" content="creat"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="生成模型的定义 生成模型是从给定的一堆数据样本中，学习数据的真正分布（由观测样本学习到真实分布），即 \(x \rightarrow p(x)\)。 作用：  任意生成一个新的样本 评估观测或者采样数据的似然度（likelihood）  现有的生成模型：  GAN：以对抗的方式从复杂的分布中采样 基于似然的生成模型，比如自回归、归一化流、VAE 变分自编码器 基于能量函数建模：把分布学习变为学习能">
<meta property="og:type" content="article">
<meta property="og:title" content="diffusion Ch01">
<meta property="og:url" content="https://ardbr.github.io/2026/02/09/notes/diffusion/diffusion-Ch01/index.html">
<meta property="og:site_name" content="creat&#39;s HomePage">
<meta property="og:description" content="生成模型的定义 生成模型是从给定的一堆数据样本中，学习数据的真正分布（由观测样本学习到真实分布），即 \(x \rightarrow p(x)\)。 作用：  任意生成一个新的样本 评估观测或者采样数据的似然度（likelihood）  现有的生成模型：  GAN：以对抗的方式从复杂的分布中采样 基于似然的生成模型，比如自回归、归一化流、VAE 变分自编码器 基于能量函数建模：把分布学习变为学习能">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ardbr.github.io/img/avatar.gif">
<meta property="article:published_time" content="2026-02-09T09:36:02.000Z">
<meta property="article:modified_time" content="2026-02-10T07:38:35.547Z">
<meta property="article:author" content="creat">
<meta property="article:tag" content="Diffusion">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ardbr.github.io/img/avatar.gif"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://ardbr.github.io/2026/02/09/notes/diffusion/diffusion-Ch01/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!true && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":10,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200,"highlightFullpage":false,"highlightMacStyle":"mac"},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: {"chs_to_cht":"已切换为繁体中文","cht_to_chs":"已切换为简体中文","day_to_night":"已切换为深色模式","night_to_day":"已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-center"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'diffusion Ch01',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/cursor.css"><link rel="stylesheet" href="/css/custom.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_4746832_eb25gdpzci6.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-image: url(/img/background.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.gif" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">37</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/categories"><i class="fa-fw fas fa-book"></i><span> 学习资料</span></a></div><div class="menus_item"><a class="site-page" href="/link"><i class="fa-fw fas fa-globe"></i><span> 常用网站</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/favicon.png" alt="Logo"><span class="site-name">creat's HomePage</span></a><a class="nav-page-title" href="/"><span class="site-name">diffusion Ch01</span></a></span><div id="music-bar"><div id="music-cover-container"><div id="music-cover"></div></div><div id="music-button-container"><div id="music-button-container-top"><span class="music-icon iconfont icon-shangyishou" id="music-prev"></span><span class="music-icon iconfont icon-bofang" id="music-playOrPause"></span><span class="music-icon iconfont icon-xiayishou" id="music-next"></span><span class="music-icon iconfont icon-liebiaoxunhuan" id="music-mode"></span></div><div id="music-button-container-bottom"><div id="music-progressBar"><span id="music-currentProgress"></span><span id="music-dot"></span></div><span id="music-progressText">00:00 / 00:00</span></div></div><div id="music-name-container"><span id="music-name"></span></div><div id="music-lyric-container"><span id="music-lyric"></span></div></div><div id="menus"><div id="toggle-menu"><span class="site-page"></span><i class="fas fa-bars fa-fw"></i></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/categories"><i class="fa-fw fas fa-book"></i><span> 学习资料</span></a></div><div class="menus_item"><a class="site-page" href="/link"><i class="fa-fw fas fa-globe"></i><span> 常用网站</span></a></div></div><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div></div></nav><div id="post-info"><h1 class="post-title">diffusion Ch01</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2026-02-09T09:36:02.000Z" title="发表于 2026-02-09 17:36:02">2026-02-09</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2026-02-10T07:38:35.547Z" title="更新于 2026-02-10 15:38:35">2026-02-10</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/notes/">notes</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/notes/diffusion/">diffusion</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">7.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>31分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h2 id="生成模型的定义">生成模型的定义</h2>
<p>生成模型是从给定的一堆数据样本中，学习数据的真正分布（由观测样本学习到真实分布），即
<span class="math inline">\(x \rightarrow p(x)\)</span>。</p>
<p>作用：</p>
<ul>
<li>任意生成一个新的样本</li>
<li>评估观测或者采样数据的似然度（likelihood）</li>
</ul>
<p>现有的生成模型：</p>
<ol type="1">
<li>GAN：以对抗的方式从复杂的分布中采样</li>
<li>基于似然的生成模型，比如自回归、归一化流、VAE 变分自编码器</li>
<li>基于能量函数建模：把分布学习变为学习能量函数</li>
<li>基于分数的生成模型：不学习能量函数，而是能量函数的分数来评判神经网络</li>
</ol>
<p>我们需要引入“潜变量”，一般记为 <span
class="math inline">\(z\)</span>。观测变量也就是数据样本 <span
class="math inline">\(x\)</span>，一般只是潜变量的一部分（某一维度）的投射。对于隐变量而言，我们一般应该寻找低维的，而不是高维的。原因有二：</p>
<ul>
<li>没有更强的先验知识，学习高维的隐变量是不可能的</li>
<li>低维隐变量的表示是一种压缩形式，语义上可能会更有意义</li>
</ul>
<h2 id="证据下界evidence-lower-bound">证据下界（Evidence Lower
Bound）</h2>
<p>我们可以通过联合密度 <span class="math inline">\(p(x, z)\)</span>
对观测样本和隐变量进行建模。而由联合密度得到似然函数 <span
class="math inline">\(p(x)\)</span> 有两种方式：</p>
<ol type="1">
<li>通过边缘概率公式：<span class="math inline">\(p(x) = \int p(x, z) \,
dz\)</span></li>
<li>通过条件概率（概率的链式法则）：<span class="math inline">\(p(x) =
\frac{p(x, z)}{p(z|x)}\)</span></li>
</ol>
<p>所谓的证据（Evidence）的叫法，实际上等价于对数似然。也就是 <span
class="math inline">\(\log p(x)\)</span>。对于我们的优化来说，目标就是
<span class="math inline">\(\max \log
p(x)\)</span>。而通过上面的公式，我们有两种方式来对 <span
class="math inline">\(p(x)\)</span> 进行表示。</p>
<p>但是，边缘概率需要对<strong>所有的</strong>隐变量 <span
class="math inline">\(z\)</span>
进行积分，这是一件很困难的事情。而且，由于真实的后验分布概率 <span
class="math inline">\(p(z|x)\)</span>
我们也是不知道的，所以应用条件概率也是一件很困难的事情，所以我们需要用一些近似的方法来估计这个后验分布概率。</p>
<figure>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/diffusion/latent_var.png" alt="latent_var" />
<figcaption aria-hidden="true">latent_var</figcaption>
</figure>
<p>我们的优化目标从 <span class="math inline">\(\max \log p(x)\)</span>
转变为 <span class="math inline">\(\max \text{ELBO}\)</span>，其中 <span
class="math inline">\(\text{ELBO} =
\mathbb{E}_{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})} \left[ \log
\frac{p(\boldsymbol{x},
\boldsymbol{z})}{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})}
\right]\)</span>，这里需要解释说明两个问题：</p>
<ol type="1">
<li><span class="math inline">\(\text{ELBO}\)</span> 确实是 <span
class="math inline">\(\log p(x)\)</span> 的一个下界，即证 <span
class="math inline">\(\log p(x) \ge
\mathbb{E}_{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})} \left[ \log
\frac{p(\boldsymbol{x},
\boldsymbol{z})}{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})}
\right]\)</span>；</li>
<li>这么多的下界中，为什么选择了 <span
class="math inline">\(\text{ELBO}\)</span> 这个下界。</li>
</ol>
<p>首先，我们来证明 <span class="math inline">\(\log p(x) \ge
\mathbb{E}_{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})} \left[ \log
\frac{p(\boldsymbol{x},
\boldsymbol{z})}{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})}
\right]\)</span>：</p>
<p><span class="math display">\[
\begin{align}
\log p(\boldsymbol{x}) &amp;= \log \int p(\boldsymbol{x},
\boldsymbol{z}) d\boldsymbol{z} &amp;&amp; \text{(Apply Equation 1)}
\tag{5} \\
&amp;= \log \int \frac{p(\boldsymbol{x}, \boldsymbol{z})
q_{\phi}(\boldsymbol{z}|\boldsymbol{x})}{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})}
d\boldsymbol{z} &amp;&amp; \text{(Multiply by } 1 =
\frac{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})}{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})}
\text{)} \tag{6} \\
&amp;= \log \mathbb{E}_{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})} \left[
\frac{p(\boldsymbol{x},
\boldsymbol{z})}{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})} \right]
&amp;&amp; \text{(Definition of Expectation)} \tag{7} \\
&amp;\ge \mathbb{E}_{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})} \left[
\log \frac{p(\boldsymbol{x},
\boldsymbol{z})}{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})} \right]
&amp;&amp; \text{(Apply Jensen&#39;s Inequality)} \tag{8}
\end{align}
\]</span></p>
<div class="note info flat"><p>注意最后一步我们用到了 Jensen 不等式，由于 <span
class="math inline">\(f(\cdot) = \log(\cdot)\)</span>
是一个凹函数（不同教材和学科方向对与凹凸函数的定义并不一致），所以有
<span class="math inline">\(f(\mathbb{E}(\cdot)) \ge
\mathbb{E}(f(\cdot))\)</span> 成立。</p>
</div>
<p>接下来说明为什么要选择 <span
class="math inline">\(\text{ELBO}\)</span> 这个下界：</p>
<p><span class="math display">\[
\begin{align}
\log p(\boldsymbol{x}) &amp;= \log p(\boldsymbol{x}) \int
q_{\phi}(\boldsymbol{z}|\boldsymbol{x}) d\boldsymbol{z} &amp;&amp;
\text{(Multiply by } 1 = \int q_{\phi}(\boldsymbol{z}|\boldsymbol{x})
d\boldsymbol{z} \text{)} \tag{9} \\
&amp;= \int q_{\phi}(\boldsymbol{z}|\boldsymbol{x}) (\log
p(\boldsymbol{x})) d\boldsymbol{z} &amp;&amp; \text{(Bring evidence into
integral)} \tag{10} \\
&amp;= \mathbb{E}_{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})} [\log
p(\boldsymbol{x})] &amp;&amp; \text{(Definition of Expectation)}
\tag{11} \\
&amp;= \mathbb{E}_{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})} \left[ \log
\frac{p(\boldsymbol{x},
\boldsymbol{z})}{p(\boldsymbol{z}|\boldsymbol{x})} \right] &amp;&amp;
\text{(Apply Equation 2)} \tag{12} \\
&amp;= \mathbb{E}_{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})} \left[ \log
\frac{p(\boldsymbol{x}, \boldsymbol{z})
q_{\phi}(\boldsymbol{z}|\boldsymbol{x})}{p(\boldsymbol{z}|\boldsymbol{x})
q_{\phi}(\boldsymbol{z}|\boldsymbol{x})} \right] &amp;&amp;
\text{(Multiply by } 1 =
\frac{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})}{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})}
\text{)} \tag{13} \\
&amp;= \mathbb{E}_{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})} \left[ \log
\frac{p(\boldsymbol{x},
\boldsymbol{z})}{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})} \right] +
\mathbb{E}_{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})} \left[ \log
\frac{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})}{p(\boldsymbol{z}|\boldsymbol{x})}
\right] &amp;&amp; \text{(Split the Expectation)} \tag{14} \\
&amp;= \mathbb{E}_{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})} \left[ \log
\frac{p(\boldsymbol{x},
\boldsymbol{z})}{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})} \right] +
D_{\text{KL}}(q_{\phi}(\boldsymbol{z}|\boldsymbol{x}) \parallel
p(\boldsymbol{z}|\boldsymbol{x})) &amp;&amp; \text{(Definition of KL
Divergence)} \tag{15} \\
&amp;\ge \mathbb{E}_{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})} \left[
\log \frac{p(\boldsymbol{x},
\boldsymbol{z})}{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})} \right]
&amp;&amp; \text{(KL Divergence always } \ge 0 \text{)} \tag{16}
\end{align}
\]</span></p>
<div class="note warning flat"><p><span class="math inline">\(\text{KL}\)</span>
散度：度量分布的接近程度，公式为：</p>
<p><span class="math inline">\(D_\text{KL}(p || q) = \int p(x) \log
\frac{p(x)}{q(x)} \text{d}x = \mathbb{E}_{p(x)} \left[ \log
\frac{p(x)}{q(x)}\right]\)</span></p>
<ul>
<li>非负性：<span class="math inline">\(p\)</span> 与 <span
class="math inline">\(q\)</span> 越接近，则 <span
class="math inline">\(\text{KL}\)</span> 散度越小，越趋近于 0。但 <span
class="math inline">\(\text{KL}\)</span> 散度非负，<span
class="math inline">\(D_\text{KL} \ge 0\)</span>。</li>
<li>非对称性：<span class="math inline">\(D_\text{KL}(p || q) \neq
D_\text{KL}(q || p)\)</span></li>
</ul>
</div>
<p>由上述的公式，我们可以得到 <span class="math display">\[
\log p(x) = \text{ELBO} + \text{KL 散度}
\]</span></p>
<p>因为 <span class="math inline">\(p(x)\)</span>
是定值常数（数据就在那里），最大化 <span
class="math inline">\(\text{ELBO}\)</span>（变分下界）就相当于是最小化
<span class="math inline">\(\text{KL}\)</span>
散度！<strong>因为我们不仅想要最大化 <span class="math inline">\(\log
p(x)\)</span>，我们还想神经网络学习的近似后验分布与真实的后验分布越接近越好（<span
class="math inline">\(\text{KL}\)</span> 散度接近于 0），所以我们选择了
<span class="math inline">\(\text{ELBO}\)</span> 这个下界。</strong></p>
<h2 id="vaevariational-autoencoder">VAE（Variational Autoencoder）</h2>
<p>接下来我们介绍什么叫做变分自编码器。这个名字听起来会特别令人困惑，什么叫做“变分自编码器”，“变分”是什么意思，“自编码器”又是什么意思呢？</p>
<h3 id="自编码器-autoencoder">自编码器 (Autoencoder)</h3>
<p>想象你是一个画家，你想把一张复杂的画（输入 <span
class="math inline">\(x\)</span>）传给你的朋友，但你只能说很简短的一句话。</p>
<ul>
<li>Encoder
(编码器)：你盯着画，提取出最核心的特征（比如“一只在草地上的白猫”），这句简短的话就是
Latent Code (隐变量 <span
class="math inline">\(z\)</span>)。这一步是压缩信息。</li>
<li>Decoder (解码器)：你的朋友听到这句话，凭空画出一张画。</li>
<li>目标：你希望朋友画出来的画，和你原本看到的那张画越像越好。在这个过程中：输入是
<span class="math inline">\(x\)</span>。输出是重建后的 <span
class="math inline">\(\hat{x}\)</span>。中间那个“瓶颈”就是 <span
class="math inline">\(z\)</span>。</li>
</ul>
<p>普通的自编码器（Autoencoder）把图片映射成一个固定的点（比如坐标 [2.5,
3.1]）</p>
<h3 id="变分-variational">变分 (Variational)</h3>
<p>“变分”这个词来自<strong>变分推断 (Variational
Inference)</strong>。为什么要“变分”？普通的自编码器有一个大问题：它生成的
<span class="math inline">\(z\)</span>
是离散的点。如果你想生成一张新图片，你不知道该去坐标系的哪里取点。如果你取在两个点中间，可能解码出来就是乱码。</p>
<p><strong>VAE
的做法：</strong>它不把图片映射成一个“点”，而是映射成一个<strong>“概率分布”
(Probability
Distribution)</strong>（通常是高斯分布，即正态分布）。Encoder
不再说：“这张图的坐标是 [2, 3]”。Encoder 改口说：“这张图大概在 [2, 3]
附近，方差是 0.5 的一个圈圈里”。这就叫“Variational
(变分)”：我们不再寻找一个确定的值，而是去寻找一个最佳的分布函数 <span
class="math inline">\(q_\phi(z|x)\)</span>
来近似那个未知的真实分布。</p>
<blockquote>
<p>一句话总结：普通自编码器是把数据压缩成代码。变分自编码器是把数据压缩成概率分布。</p>
</blockquote>
<p>下面将结合公式进行进一步的说明。</p>
<p><span class="math display">\[
\begin{align}
\mathbb{E}_{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})} \left[ \log
\frac{p(\boldsymbol{x},
\boldsymbol{z})}{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})} \right] &amp;=
\mathbb{E}_{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})} \left[ \log
\frac{p_{\theta}(\boldsymbol{x}|\boldsymbol{z})p(\boldsymbol{z})}{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})}
\right] &amp;&amp; \text{(Chain Rule of Probability)} \\
&amp;= \mathbb{E}_{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})} [\log
p_{\theta}(\boldsymbol{x}|\boldsymbol{z})] +
\mathbb{E}_{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})} \left[ \log
\frac{p(\boldsymbol{z})}{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})}
\right] &amp;&amp; \text{(Split the Expectation)} \\
&amp;= \underbrace{\mathbb{E}_{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})}
[\log p_{\theta}(\boldsymbol{x}|\boldsymbol{z})]}_{\text{reconstruction
term}} -
\underbrace{D_{\text{KL}}(q_{\phi}(\boldsymbol{z}|\boldsymbol{x})
\parallel p(\boldsymbol{z}))}_{\text{prior matching term}} &amp;&amp;
\text{(Definition of KL Divergence)}
\end{align}
\]</span></p>
<p>VAE 的训练目标就是让这个式子最大化，它由两部分对抗组成：</p>
<ul>
<li><p>第一部分：<strong>Reconstruction Term
(重建项)对应概念：Autoencoder (自编码器)</strong></p>
<p>含义：“画得像不像？”我们从分布里采样一个 <span
class="math inline">\(z\)</span>，通过 Decoder 生成图像，<mark class="hl-label orange">希望它和原图越像越好</mark>。
<div class="note danger flat"><p>测量编码器 <span class="math inline">\(p_\theta(x | z)\)</span>
重建（给定隐变量 <span class="math inline">\(z\)</span> 生成原数据 <span
class="math inline">\(x\)</span> 的过程）可能性</p>
</div></p></li>
<li><p>第二部分：<strong>Prior Matching Term (先验匹配项 / <span
class="math inline">\(\text{KL}\)</span> 散度)对应概念：Variational
(变分)</strong></p>
<p>含义：“分布规不规范？”这里用到了 <span
class="math inline">\(\text{KL}\)</span>
散度！它强迫神经网络预测出来的分布 <span
class="math inline">\(q_\phi(z|x)\)</span> 去接近一个标准的正态分布
<span
class="math inline">\(p(z)\)</span>（通常是均值为0，方差为1的标准高斯分布）。为什么要做这一步？
为了让生成的“潜空间” (Latent Space)
连续且平滑。这样你以后在这个空间里随便取一个点，都能生成出像模像样的图片，而不是乱码。
<div class="note danger flat"><p>度量所学习到的 <span class="math inline">\(q_\phi(z | x)\)</span>
能够保留多少原先的先验知识 <span class="math inline">\(p(z)\)</span>
的程度</p>
</div></p></li>
</ul>
<p>我们需要使得第一部分尽可能大，第二部分尽可能小。<strong>为了简便起见，我们一般都会假设
<span class="math inline">\(p_\theta(x | z)\)</span>、<span
class="math inline">\(q_\phi(z | x)\)</span> 和 <span
class="math inline">\(p(z)\)</span>
都是服从高斯分布的。</strong>后验分布 <span
class="math inline">\(q_{\phi}(\boldsymbol{z}|\boldsymbol{x})\)</span>
和先验分布 <span class="math inline">\(p(\boldsymbol{z})\)</span>
的公式见下方：</p>
<p><span class="math display">\[
\begin{align}
q_{\phi}(\boldsymbol{z}|\boldsymbol{x}) &amp;=
\mathcal{N}(\boldsymbol{z}; \boldsymbol{\mu}_{\phi}(\boldsymbol{x}),
\boldsymbol{\sigma}_{\phi}^2(\boldsymbol{x})\mathbf{I}) \\
p(\boldsymbol{z}) &amp;= \mathcal{N}(\boldsymbol{z}; \mathbf{0},
\mathbf{I})
\end{align}
\]</span></p>
<blockquote>
<p>为了更加简便，我们甚至不会考虑协方差，只是考虑每一个高斯分布单独的方差。</p>
</blockquote>
<p><span class="math display">\[
\operatorname*{arg\,max}_{\phi, \theta}
\mathbb{E}_{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})} \left[ \log
p_{\theta}(\boldsymbol{x}|\boldsymbol{z}) \right] -
D_{\text{KL}}(q_{\phi}(\boldsymbol{z}|\boldsymbol{x}) \parallel
p(\boldsymbol{z})) \approx \operatorname*{arg\,max}_{\phi, \theta}
\sum_{l=1}^{L} \log p_{\theta}(\boldsymbol{x}|\boldsymbol{z}^{(l)}) -
D_{\text{KL}}(q_{\phi}(\boldsymbol{z}|\boldsymbol{x}) \parallel
p(\boldsymbol{z}))
\]</span></p>
<p>在上面这个式子中，我们是可以直接计算出 <span
class="math inline">\(D_{\text{KL}}(q_{\phi}(\boldsymbol{z}|\boldsymbol{x})
\parallel p(\boldsymbol{z}))\)</span> 这一项的，因为 <span
class="math inline">\(\parallel\)</span>
两边的都是高斯分布，因此可以直接代入相关的公式进行计算。</p>
<p>对于第一项求期望，我们可以有两种方法：蒙特卡洛和参数重整化方法</p>
<h3 id="蒙特卡洛方法">蒙特卡洛方法</h3>
<p>蒙特卡洛方法就是不断地采样，然后取平均值来近似估计真实值。具体来说，对于
<span
class="math inline">\(\mathbb{E}_{p(\boldsymbol{x})}(g(\boldsymbol{x}))\)</span>，第一步我们需要以
<span class="math inline">\(p(\boldsymbol{x})\)</span>
的概率分布来采样多个（记为 <span class="math inline">\(n\)</span>）
<span class="math inline">\(\boldsymbol{x}\)</span>，即 <span
class="math inline">\(\{\boldsymbol{x_i}\}^n_{i=1}\)</span>。然后，对于这
<span class="math inline">\(n\)</span> 个 <span
class="math inline">\(\boldsymbol{x}_i\)</span>，我们取其平均值 <span
class="math inline">\(\frac{1}{n}\sum^n_{i=1}g(\boldsymbol{x}_i)\)</span>，作为对于期望的估计值。</p>
<p>在前面的式子中，我们就是采样了 <span class="math inline">\(L\)</span>
个样本，之后求和取平均。但是因为这里我们优化的目标和 <span
class="math inline">\(L\)</span> 无关，因此是否除以 <span
class="math inline">\(L\)</span>
关系并不大，为了简便起见，我们并没有除以 <span
class="math inline">\(L\)</span>。</p>
<h4 id="蒙特卡洛随机采样的问题">蒙特卡洛随机采样的问题</h4>
<mark class="hl-label red">蒙特卡洛有一个致命的问题，那就是随机采样是不可微分的，进而会导致梯度传播、计算失效。</mark>
<p>我们可以从直观理解和数学原理两个角度来看。</p>
<ol type="1">
<li><p><strong>直观理解：</strong>断裂的计算图在神经网络训练中，为了更新参数（比如
<span class="math inline">\(\mu\)</span> 和 <span
class="math inline">\(\sigma\)</span>），我们需要计算梯度（Gradient）。梯度的本质是问这样一个问题：</p>
<blockquote>
<p>“如果我把输入（<span class="math inline">\(\mu\)</span> 或 <span
class="math inline">\(\sigma\)</span>）稍微增加一点点，输出（Loss）会变化多少？”</p>
</blockquote>
<p>正常情况（可微分）假设公式是 <span class="math inline">\(z = \mu +
\sigma\)</span>。如果你把 <span class="math inline">\(\mu\)</span> 增加
0.001，<span class="math inline">\(z\)</span> 也会增加
0.001。这种关系是确定性的、连续的。计算机可以算出导数 <span
class="math inline">\(\frac{\partial z}{\partial \mu} =
1\)</span>，梯度畅通无阻。</p>
<p>随机采样的情况（不可微分）现在公式变成了 <span
class="math inline">\(z \sim \mathcal{N}(\mu,
\sigma^2)\)</span>（从分布中采样）。</p>
<ul>
<li><strong>前向传播（Forward）：</strong>你告诉电脑 <span
class="math inline">\(\mu=0,
\sigma=1\)</span>，电脑扔了个骰子，给你一个数字 <span
class="math inline">\(z = 0.5\)</span>。</li>
<li><strong>反向传播（Backward）：</strong>现在你想算梯度。你问电脑：“如果我把
<span class="math inline">\(\mu\)</span> 从 0 变成 0.001，刚才那个 <span
class="math inline">\(z=0.5\)</span>
会变成多少？”电脑的回答：“我不知道。因为 <span
class="math inline">\(z\)</span> 是我随机抽出来的。如果你改了 <span
class="math inline">\(\mu\)</span>，那就是另一次抽签了，出来的可能是
<span class="math inline">\(0.6\)</span>，也可能是 <span
class="math inline">\(-0.2\)</span>。这两个数字之间没有平滑的过渡关系。”</li>
</ul>
<p>因此，采样的结果是一个随机的数值，而不是一个包含参数 <span
class="math inline">\(\mu\)</span> 和 <span
class="math inline">\(\sigma\)</span> 的函数。在计算图（Computation
Graph）中，采样操作是一个切断点。</p></li>
<li><p><strong>数学解释：</strong>链式法则的失效即使我们强行去算，也会发现数学上走不通。假设我们的损失函数是
<span class="math inline">\(L\)</span>，我们需要求 <span
class="math inline">\(\frac{\partial L}{\partial
\mu}\)</span>。根据链式法则：</p>
<p><span class="math display">\[
\frac{\partial L}{\partial \mu} = \frac{\partial L}{\partial z} \cdot
\frac{\partial z}{\partial \mu}
\]</span> <span class="math inline">\(\frac{\partial L}{\partial
z}\)</span>：这一步没问题，神经网络后面部分都是可导的。</p>
<p><span class="math inline">\(\frac{\partial z}{\partial
\mu}\)</span>：这一步是关键。<span class="math inline">\(z\)</span> 对
<span class="math inline">\(\mu\)</span> 的导数是多少？当你执行 <span
class="math inline">\(z = \text{sample}(\mu, \sigma)\)</span>
后，在计算机眼里，<span class="math inline">\(z\)</span>
就变成了一个常数（比如 0.53）。常数对任何变量的导数都是 0。 <span
class="math display">\[
\frac{\partial z}{\partial \mu} = 0
\]</span>如果这一项是 0，那么整个梯度 <span
class="math inline">\(\frac{\partial L}{\partial \mu}\)</span>
也就变成了 0。这意味着你的神经网络根本学不到东西，因为它觉得 <span
class="math inline">\(\mu\)</span> 和 <span
class="math inline">\(\sigma\)</span> 怎么变都不会影响 <span
class="math inline">\(z\)</span>（因为采样操作把这种联系掩盖了）。</p></li>
</ol>
<h3 id="参数重整化方法">参数重整化方法</h3>
<p>上述问题的解决方案就是<strong>重参数化技巧 (Reparameterization
Trick)</strong>。</p>
<p>为了解决这个问题，VAE
发明了天才般的重参数化技巧。它的核心思想是：把“随机性”从网络的主干道上剥离出去，把它变成一个外挂的输入。我们不再直接采
<span class="math inline">\(z \sim \mathcal{N}(\mu,
\sigma^2)\)</span>，而是：先从一个标准正态分布（没有参数，<span
class="math inline">\(\mu=0, \sigma=1\)</span>）中采一个噪声 <span
class="math inline">\(\epsilon\)</span>： <span class="math display">\[
\epsilon \sim \mathcal{N}(0, 1)
\]</span> (注意：这一步是随机的，但它不依赖于 <span
class="math inline">\(\mu\)</span> 和 <span
class="math inline">\(\sigma\)</span>，所以不需要对它求导)然后通过一个确定性的公式计算
<span class="math inline">\(z\)</span>： <span class="math display">\[
z = \mu + \sigma \cdot \epsilon
\]</span> 现在看看有什么不同：当我们再问：“如果 <span
class="math inline">\(\mu\)</span> 增加一点点，<span
class="math inline">\(z\)</span> 会怎么变？” <span
class="math display">\[\frac{\partial z}{\partial \mu} = \frac{\partial
(\mu + \sigma \cdot \epsilon)}{\partial \mu} = 1
\]</span> <span class="math display">\[
\frac{\partial z}{\partial \sigma} = \frac{\partial (\mu + \sigma \cdot
\epsilon)}{\partial \sigma} = \epsilon
\]</span> 通了！<span class="math inline">\(\epsilon\)</span>
在这里被当作一个常数（就像输入数据一样）。<span
class="math inline">\(z\)</span> 变成了 <span
class="math inline">\(\mu\)</span> 和 <span
class="math inline">\(\sigma\)</span> 的平滑函数。梯度可以通过 <span
class="math inline">\(\times\)</span> 和 <span
class="math inline">\(+\)</span> 运算流畅地传回到 <span
class="math inline">\(\mu\)</span> 和 <span
class="math inline">\(\sigma\)</span>。</p>
<div class="note success flat"><p>将随机变量重新表示为噪声变量的确定性函数： <span
class="math display">\[
\boldsymbol{z} = \boldsymbol{\mu}_{\phi}(\boldsymbol{x}) +
\boldsymbol{\sigma}_{\phi}(\boldsymbol{x}) \odot \boldsymbol{\epsilon}
\quad \text{with} \quad \boldsymbol{\epsilon} \sim
\mathcal{N}(\boldsymbol{\epsilon}; \mathbf{0}, \mathbf{I})
\]</span></p>
</div>
<h2 id="hvaehierarchical-variational-autoencoder">HVAE（Hierarchical
Variational Autoencoder）</h2>
<p>在前面，我们考虑了观测样本 <span
class="math inline">\(\boldsymbol{x}\)</span> 与单个隐变量 <span
class="math inline">\(\boldsymbol{z}\)</span>
之间的关系。但是，一个隐变量可能还会与其他的隐变量之间有关系，如下图所示：</p>
<figure>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/diffusion/HVAE.png" alt="alt text" />
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>为了简便起见，我们只考虑具有马尔可夫性质的 <span
class="math inline">\(\text{HVAE}\)</span>，称作 <span
class="math inline">\(\text{MHVAE}\)</span>。也就是对于 <span
class="math inline">\(\boldsymbol{z}_i\)</span>
来说，它只会依赖于它前一个的隐变量（或观测变量）的状态，而不是历史上所有前面的隐变量。</p>
<h3 id="优化目标">优化目标</h3>
<p>在 VAE 中，我们的优化目标是最大化 ELBO。VAE
中只有两个变量，一个是观测数据 <span
class="math inline">\(\boldsymbol{x}\)</span>，一个是 <span
class="math inline">\(\boldsymbol{z}\)</span>，因此可以看做是 2
维的。</p>
<p>但是在 MHVAE
中，我们的隐变量个数增多了，因此维度也会上去。其联合概率密度分布（解码
decoder 过程）与近似的后验概率分布（编码 encoder
过程）的公式见下方：</p>
<p><span class="math display">\[
\begin{align}
p(\boldsymbol{x}, \boldsymbol{z}_{1:T}) &amp;=
p(\boldsymbol{z}_T)p_{\theta}(\boldsymbol{x}|\boldsymbol{z}_1)
\prod_{t=2}^{T} p_{\theta}(\boldsymbol{z}_{t-1}|\boldsymbol{z}_t)
\label{eq:mhvae_decoder}\\
q_{\phi}(\boldsymbol{z}_{1:T}|\boldsymbol{x}) &amp;=
q_{\phi}(\boldsymbol{z}_1|\boldsymbol{x}) \prod_{t=2}^{T}
q_{\phi}(\boldsymbol{z}_t|\boldsymbol{z}_{t-1}) \label{eq:mhvae_encoder}
\end{align}
\]</span></p>
<p>其 ELBO 的公式推导见下方： <span class="math display">\[
\begin{align}
\log p(\boldsymbol{x}) &amp;= \log \int p(\boldsymbol{x},
\boldsymbol{z}_{1:T}) d\boldsymbol{z}_{1:T} &amp;&amp; \text{(Apply
Equation 1)} \\
&amp;= \log \int \frac{p(\boldsymbol{x},
\boldsymbol{z}_{1:T})q_{\phi}(\boldsymbol{z}_{1:T}|\boldsymbol{x})}{q_{\phi}(\boldsymbol{z}_{1:T}|\boldsymbol{x})}
d\boldsymbol{z}_{1:T} &amp;&amp; \text{(Multiply by } 1 =
\frac{q_{\phi}(\boldsymbol{z}_{1:T}|\boldsymbol{x})}{q_{\phi}(\boldsymbol{z}_{1:T}|\boldsymbol{x})}
\text{)} \\
&amp;= \log \mathbb{E}_{q_{\phi}(\boldsymbol{z}_{1:T}|\boldsymbol{x})}
\left[ \frac{p(\boldsymbol{x},
\boldsymbol{z}_{1:T})}{q_{\phi}(\boldsymbol{z}_{1:T}|\boldsymbol{x})}
\right] &amp;&amp; \text{(Definition of Expectation)} \\
&amp;\ge \mathbb{E}_{q_{\phi}(\boldsymbol{z}_{1:T}|\boldsymbol{x})}
\left[ \log \frac{p(\boldsymbol{x},
\boldsymbol{z}_{1:T})}{q_{\phi}(\boldsymbol{z}_{1:T}|\boldsymbol{x})}
\right] &amp;&amp; \text{(Apply Jensen&#39;s Inequality)}
\end{align}
\]</span></p>
<p>注意，我们可以利用马尔可夫的性质，即当前时刻的状态仅依赖于上一时刻，所以将其进一步化简，也就是代入公式
<span class="math inline">\(\ref{eq:mhvae_encoder}\)</span> 和 <span
class="math inline">\(\ref{eq:mhvae_decoder}\)</span>： <span
class="math display">\[
\begin{equation} \label{eq:vdm}
\mathbb{E}_{q_{\phi}(\boldsymbol{z}_{1:T}|\boldsymbol{x})} \left[ \log
\frac{p(\boldsymbol{x},
\boldsymbol{z}_{1:T})}{q_{\phi}(\boldsymbol{z}_{1:T}|\boldsymbol{x})}
\right] = \mathbb{E}_{q_{\phi}(\boldsymbol{z}_{1:T}|\boldsymbol{x})}
\left[ \log
\frac{p(\boldsymbol{z}_T)p_{\boldsymbol{\theta}}(\boldsymbol{x}|\boldsymbol{z}_1)
\prod_{t=2}^{T}
p_{\boldsymbol{\theta}}(\boldsymbol{z}_{t-1}|\boldsymbol{z}_t)}{q_{\phi}(\boldsymbol{z}_1|\boldsymbol{x})
\prod_{t=2}^{T} q_{\phi}(\boldsymbol{z}_t|\boldsymbol{z}_{t-1})} \right]
\end{equation}
\]</span></p>
<p>之后，我们会进一步围绕公式 <span
class="math inline">\(\ref{eq:vdm}\)</span>
展开<strong>变分扩散模型</strong>的研究。</p>
<h2 id="vdmvariational-diffusion-model">VDM（Variational Diffusion
Model）</h2>
<p>扩散模型可以看做是一种特殊的 MHVAE，但是会有三个关键限制。</p>
<ul>
<li><p><strong>隐变量维度等于数据维度</strong></p>
<p><strong>普通 VAE：</strong>通常会把图片“压缩”。比如一张
1000像素的图，压缩成一个 10个数字的向量（隐变量），这叫降维。</p>
<p><strong>扩散模型：</strong>不压缩。如果你的原图是 <span
class="math inline">\(256 \times 256\)</span>
的，那么中间的每一层噪声图（隐变量）也是 <span class="math inline">\(256
\times 256\)</span> 的。图片的大小始终保持不变，只是内容变了。</p></li>
<li><p><strong>编码器结构是固定的，不需要学习</strong></p>
<p><strong>普通
VAE：</strong>编码器（Encoder）是一个神经网络，需要通过训练来学习如何提取特征。</p>
<p><strong>扩散模型：</strong>编码器（也就是“前向过程”或“加噪过程”）是写死的数学规则。它不需要神经网络去学，只是机械地向图片里加高斯噪声（Linear
Gaussian）。</p></li>
<li><p><strong>最终变为标准高斯分布</strong></p>
<p><strong>普通
VAE：</strong>我们希望隐变量符合高斯分布，但这是一种约束。
<strong>扩散模型：</strong>这是一个硬性终点。随着时间 <span
class="math inline">\(t\)</span> 推移，噪声越加越多，直到最后时刻 <span
class="math inline">\(T\)</span>，原本的图片彻底变成了一张纯粹的随机噪声图（标准高斯分布，均值为0，方差为1）。这意味着我们清楚地知道“终点”长什么样。</p></li>
</ul>
<div class="note success flat"><p><strong>扩散模型 = 马尔可夫层次变分自编码器 (MHVAE) +
3个限制条件</strong></p>
</div>
<h3 id="限制一的数学建模">限制一的数学建模</h3>
<p>在第一个限制的基础上，我们可以改写公式 <span
class="math inline">\(\ref{eq:mhvae_encoder}\)</span>，把其中的 <span
class="math inline">\(\boldsymbol{z}\)</span> 全部换成 <span
class="math inline">\(\boldsymbol{x}\)</span>，也就是下面这个式子 <span
class="math inline">\(\ref{eq:vdm_encoder}\)</span>。 <span
class="math display">\[
\begin{equation} \label{eq:vdm_encoder}
q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0) = \prod_{t=1}^{T}
q(\boldsymbol{x}_t|\boldsymbol{x}_{t-1})
\end{equation}
\]</span></p>
<h3 id="限制二的数学建模">限制二的数学建模</h3>
<p>在第二个限制的基础上，因为前向编码过程是高斯建模，非学习性质的，因此，我们可以给出迭代形式的具体的编码公式，如式
<span class="math inline">\(\ref{eq:vdm_encoder_iter}\)</span>
所示。注意，这个公式里面只有 <span
class="math inline">\(\alpha_t\)</span>
是未知的，但是，我们可以人为预先设置好每一步的 <span
class="math inline">\(\alpha_t\)</span>，就像学习率一样。这个时候，<strong>编码过程是不含参数，唯一确定的。</strong></p>
<p><span class="math display">\[
\begin{equation} \label{eq:vdm_encoder_iter}
q(\boldsymbol{x}_t|\boldsymbol{x}_{t-1}) = \mathcal{N}(\boldsymbol{x}_t;
\sqrt{\alpha_t}\boldsymbol{x}_{t-1}, (1-\alpha_t)\mathbf{I})
\end{equation}
\]</span></p>
<h3 id="限制三的数学建模">限制三的数学建模</h3>
<p>在第三个限制的基础上，最终的 <span
class="math inline">\(\boldsymbol{x}_T\)</span> 服从高斯分布。因此，有式
<span class="math inline">\(\ref{eq:vdm_encoder_xT}\)</span> 成立。</p>
<p><span class="math display">\[
\begin{align}
p(\boldsymbol{x}_{0:T}) &amp;= p(\boldsymbol{x}_T) \prod_{t=1}^{T}
p_{\boldsymbol{\theta}}(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t) \\
\text{where,} &amp; \notag \\
p(\boldsymbol{x}_T) &amp;= \mathcal{N}(\boldsymbol{x}_T; \mathbf{0},
\mathbf{I}) \label{eq:vdm_encoder_xT}
\end{align}
\]</span></p>
<h3 id="总体-pipeline">总体 pipeline</h3>
<ol type="1">
<li>Encoder：不设置参数，全部都建模为高斯分布；</li>
<li>Decoder：只需关注去噪过程，其中 <span
class="math inline">\(p_\theta(\boldsymbol{x}_{t-1} |
\boldsymbol{x}_t)\)</span> 是通过模型训练参数 <span
class="math inline">\(\theta\)</span> 进行学习的。</li>
</ol>
<p>训练好一个 VDM 模型，从高斯噪声 <span
class="math inline">\(p(\boldsymbol{x}_T)\)</span> 中采样得到纯噪声图片
<span class="math inline">\(\boldsymbol{x}_T\)</span>。然后迭代去噪
<span class="math inline">\(T\)</span> 步，每一步使用模型训练出的 <span
class="math inline">\(p_\theta(\boldsymbol{x}_{t-1} |
\boldsymbol{x}_t)\)</span> 去噪。</p>
<figure>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/diffusion/vdm.png" alt="vdm" />
<figcaption aria-hidden="true">vdm</figcaption>
</figure>
<h3 id="vdm-优化目标">VDM 优化目标</h3>
<p>依旧是之前的 <span
class="math inline">\(\text{ELBO}\)</span>，但是当我们把所有的 <span
class="math inline">\(\boldsymbol{z}\)</span> 替换成 <span
class="math inline">\(\boldsymbol{x}\)</span>
之后，可以进行进一步的处理了。</p>
<p><span class="math display">\[
\begin{align}
\log p(\boldsymbol{x}) &amp;= \log \int p(\boldsymbol{x}_{0:T})
d\boldsymbol{x}_{1:T} \\
&amp;= \log \int
\frac{p(\boldsymbol{x}_{0:T})q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)}{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)}
d\boldsymbol{x}_{1:T} \\
&amp;= \log \mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[
\frac{p(\boldsymbol{x}_{0:T})}{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)}
\right] \\
&amp;\geq \mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[
\log
\frac{p(\boldsymbol{x}_{0:T})}{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)}
\right] \quad \\
&amp;= \mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log
\frac{p(\boldsymbol{x}_T) \prod_{t=1}^T
p_{\boldsymbol{\theta}}(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t)}{\prod_{t=1}^T
q(\boldsymbol{x}_t|\boldsymbol{x}_{t-1})} \right] \\
&amp;= \mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log
\frac{p(\boldsymbol{x}_T)
p_{\boldsymbol{\theta}}(\boldsymbol{x}_0|\boldsymbol{x}_1) \prod_{t=2}^T
p_{\boldsymbol{\theta}}(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t)}{q(\boldsymbol{x}_T|\boldsymbol{x}_{T-1})
\prod_{t=1}^{T-1} q(\boldsymbol{x}_t|\boldsymbol{x}_{t-1})} \right] \\
&amp;= \mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log
\frac{p(\boldsymbol{x}_T)
p_{\boldsymbol{\theta}}(\boldsymbol{x}_0|\boldsymbol{x}_1)
\prod_{t=1}^{T-1}
p_{\boldsymbol{\theta}}(\boldsymbol{x}_t|\boldsymbol{x}_{t+1})}{q(\boldsymbol{x}_T|\boldsymbol{x}_{T-1})
\prod_{t=1}^{T-1} q(\boldsymbol{x}_t|\boldsymbol{x}_{t-1})} \right] \\
&amp;= \mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[
\frac{p(\boldsymbol{x}_T)p_{\boldsymbol{\theta}}(\boldsymbol{x}_0|\boldsymbol{x}_1)}{q(\boldsymbol{x}_T|\boldsymbol{x}_{T-1})}
\right] + \mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[
\log \prod_{t=1}^{T-1}
\frac{p_{\boldsymbol{\theta}}(\boldsymbol{x}_t|\boldsymbol{x}_{t+1})}{q(\boldsymbol{x}_t|\boldsymbol{x}_{t-1})}
\right] \\
&amp;= \mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} [\log
p_{\boldsymbol{\theta}}(\boldsymbol{x}_0|\boldsymbol{x}_1)] +
\mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log
\frac{p(\boldsymbol{x}_T)}{q(\boldsymbol{x}_T|\boldsymbol{x}_{T-1})}
\right] + \mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[
\sum_{t=1}^{T-1} \log
\frac{p_{\boldsymbol{\theta}}(\boldsymbol{x}_t|\boldsymbol{x}_{t+1})}{q(\boldsymbol{x}_t|\boldsymbol{x}_{t-1})}
\right] \\
&amp;= \mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} [\log
p_{\boldsymbol{\theta}}(\boldsymbol{x}_0|\boldsymbol{x}_1)] +
\mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log
\frac{p(\boldsymbol{x}_T)}{q(\boldsymbol{x}_T|\boldsymbol{x}_{T-1})}
\right] + \sum_{t=1}^{T-1}
\mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log
\frac{p_{\boldsymbol{\theta}}(\boldsymbol{x}_t|\boldsymbol{x}_{t+1})}{q(\boldsymbol{x}_t|\boldsymbol{x}_{t-1})}
\right] \\
&amp;= \mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} [\log
p_{\boldsymbol{\theta}}(\boldsymbol{x}_0|\boldsymbol{x}_1)] +
\mathbb{E}_{q(\boldsymbol{x}_{T-1}, \boldsymbol{x}_T|\boldsymbol{x}_0)}
\left[ \log
\frac{p(\boldsymbol{x}_T)}{q(\boldsymbol{x}_T|\boldsymbol{x}_{T-1})}
\right] + \sum_{t=1}^{T-1} \mathbb{E}_{q(\boldsymbol{x}_{t-1},
\boldsymbol{x}_t, \boldsymbol{x}_{t+1}|\boldsymbol{x}_0)} \left[ \log
\frac{p_{\boldsymbol{\theta}}(\boldsymbol{x}_t|\boldsymbol{x}_{t+1})}{q(\boldsymbol{x}_t|\boldsymbol{x}_{t-1})}
\right] \\
&amp;= \underbrace{\mathbb{E}_{q(\boldsymbol{x}_1|\boldsymbol{x}_0)}
[\log
p_{\boldsymbol{\theta}}(\boldsymbol{x}_0|\boldsymbol{x}_1)]}_{\text{reconstruction
term}} -
\underbrace{\mathbb{E}_{q(\boldsymbol{x}_{T-1}|\boldsymbol{x}_0)}
[D_{\text{KL}}(q(\boldsymbol{x}_T|\boldsymbol{x}_{T-1}) \parallel
p(\boldsymbol{x}_T))]}_{\text{prior matching term}} \\
&amp; - \underbrace{\sum_{t=1}^{T-1} \mathbb{E}_{q(\boldsymbol{x}_{t-1},
\boldsymbol{x}_{t+1}|\boldsymbol{x}_0)}
[D_{\text{KL}}(q(\boldsymbol{x}_t|\boldsymbol{x}_{t-1}) \parallel
p_{\theta}(\boldsymbol{x}_t|\boldsymbol{x}_{t+1}))]}_{\text{consistency
term}}
\end{align}
\]</span></p>
<p>上面的推导中关于 prior matching term
这一部分比较难以理解，下面具体展开来说明。</p>
<p>$$ <span class="math display">\[\begin{align}
\mathbb{E}_{q(\boldsymbol{x}_{T-1}, \boldsymbol{x}_T|\boldsymbol{x}_0)}
\left[ \log
\frac{p(\boldsymbol{x}_T)}{q(\boldsymbol{x}_T|\boldsymbol{x}_{T-1})}
\right]

&amp;= \int \log \frac{p(\boldsymbol{x}_T)}{q(\boldsymbol{x}_T |
\boldsymbol{x}_{T-1})} q(\boldsymbol{x}_{T-1}, \boldsymbol{x}_T |
\boldsymbol{x}_0) \text{d}\boldsymbol{x}_{T-1:T} \\

&amp;= \int \log \frac{p(\boldsymbol{x}_T)}{q(\boldsymbol{x}_T |
\boldsymbol{x}_{T-1})} q(\boldsymbol{x}_{T} | \boldsymbol{x}_{T-1},
\boldsymbol{x}_0) q(\boldsymbol{x}_{T-1} | \boldsymbol{x}_0)
\text{d}\boldsymbol{x}_{T-1:T} \\

&amp;= - \int \left[ \log \frac{q(\boldsymbol{x}_T |
\boldsymbol{x}_{T-1})}{p(\boldsymbol{x}_T)} q(\boldsymbol{x}_{T} |
\boldsymbol{x}_{T-1}) \text{d} (\boldsymbol{x}_T) \right]
q(\boldsymbol{x}_{T-1} | \boldsymbol{x}_0) \text{d} \boldsymbol{x}_{T-1}
\\

&amp;= -\int D_\text{KL}(q(\boldsymbol{x}_T | \boldsymbol{x}_{T-1})
\parallel p(\boldsymbol{x}_T)) q(\boldsymbol{x}_{T-1} |
\boldsymbol{x}_0) \text{d} \boldsymbol{x}_{T-1} \\

&amp;= \mathbb{E}_{q(\boldsymbol{x}_{T-1}|\boldsymbol{x}_0)}
D_{\text{KL}}(q(\boldsymbol{x}_T|\boldsymbol{x}_{T-1}) \parallel
p(\boldsymbol{x}_T))
\end{align}\]</span> $$</p>
<p>关键之处在于理解 <span class="math inline">\(\text{KL}\)</span>
散度的定义式，以及联合的期望如何进行拆解。我们从开始的 <span
class="math inline">\(\mathbb{E}_{q(\boldsymbol{x}_{T-1},
\boldsymbol{x}_T|\boldsymbol{x}_0)}\)</span> 期望变成了最终的 <span
class="math inline">\(\mathbb{E}_{q(\boldsymbol{x}_{T-1} |
\boldsymbol{x}_0)}\)</span>，成功地把 <span
class="math inline">\(\boldsymbol{x}_T\)</span> 提取了出来，最终放进了
<span class="math inline">\(\text{KL}\)</span> 散度里面。</p>
<div class="note danger flat"><p>📖采用对数似然是为了数值稳定性，能够在保证成千上万的像素点的概率相乘之后，总的概率不至于太小。因为概率的连乘反映到对数运算中是相加的形式！</p>
</div>
<ul>
<li><span
class="math inline">\(\mathbb{E}_{q(\boldsymbol{x}_1|\boldsymbol{x}_0)}
[\log
p_{\boldsymbol{\theta}}(\boldsymbol{x}_0|\boldsymbol{x}_1)]\)</span>
被称作重建项，这一项的含义是评估加噪一步后的 <span
class="math inline">\(\boldsymbol{x}_1\)</span> 与原来的观测数据 <span
class="math inline">\(\boldsymbol{x}_0\)</span>
之间的对数似然。我们需要最大化对数似然，本质就是在给定 <span
class="math inline">\(\boldsymbol{x}_1\)</span>
这个稍微带一点噪声的数据基础上，我们恢复出 <span
class="math inline">\(\boldsymbol{x}_0\)</span> 的概率分布。</li>
<li><span
class="math inline">\(\mathbb{E}_{q(\boldsymbol{x}_{T-1}|\boldsymbol{x}_0)}
[D_{\text{KL}}(q(\boldsymbol{x}_T|\boldsymbol{x}_{T-1}) \parallel
p(\boldsymbol{x}_T))]\)</span>
这一项被称作先验匹配项。它的意思是我们最后一步加噪由 <span
class="math inline">\(\boldsymbol{x}_{T-1}\)</span> 生成的 <span
class="math inline">\(\boldsymbol{x}_T\)</span> 应该要与标准高斯噪声
<span class="math inline">\(\boldsymbol{x}_T\)</span>
的分布足够接近。对于这一项来说，我们并不需要去训练，因为这一项也没有任何参数需要去学习。我们只需要保证
<span class="math inline">\(T\)</span>
足够大即可，这样这一项就能够趋近于 0。</li>
<li><span class="math inline">\(\sum_{t=1}^{T-1}
\mathbb{E}_{q(\boldsymbol{x}_{t-1},
\boldsymbol{x}_{t+1}|\boldsymbol{x}_0)}
[D_{\text{KL}}(q(\boldsymbol{x}_t|\boldsymbol{x}_{t-1}) \parallel
p_{\theta}(\boldsymbol{x}_t|\boldsymbol{x}_{t+1}))]\)</span>
是一致项。它所要说明的意思是对于每一时间步 <span
class="math inline">\(t\)</span>，我们的加噪过程和去噪过程应该保持一致性。也就是我们从
<span class="math inline">\(\boldsymbol{x}_{t-1}\)</span> 加噪获取到的
<span class="math inline">\(\boldsymbol{x}_{t}\)</span>，应该要与我们从
<span class="math inline">\(\boldsymbol{x}_{t+1}\)</span> 去噪获取到的
<span class="math inline">\(\boldsymbol{x}_{t}\)</span>，这两个的 <span
class="math inline">\(\boldsymbol{x}_{t}\)</span>
应该要越接近越好。这样就能够保证加噪和去噪的每一时间步的 <span
class="math inline">\(\boldsymbol{x}_t\)</span>
是一致的。同时从这里我们也可以看出，这一项需要中间所有时间步的 <span
class="math inline">\(\text{KL}\)</span>
散度求和，因此会是整个训练过程最耗时的部分。</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://Ardbr.github.io">creat</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://ardbr.github.io/2026/02/09/notes/diffusion/diffusion-Ch01/">https://ardbr.github.io/2026/02/09/notes/diffusion/diffusion-Ch01/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://Ardbr.github.io" target="_blank">creat's HomePage</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Diffusion/">Diffusion</a></div><div class="post-share"><div class="social-share" data-image="/img/avatar.gif" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2026/02/10/notes/diffusion/diffusion-Ch02/" title="diffusion Ch02"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">diffusion Ch02</div></div><div class="info-2"><div class="info-item-1">在上一章中，我们讲了 VDM 的第一种拆解方式，我们最终把 \(\text{ELBO}\) 拆解成了 3 项，分别是重建项、先验匹配项和一致项。这一种拆解的特点可总结如下：  三项都有期望，因此可以用蒙特卡洛方法来进行估计； “一致项”在时间步 \(t\) 的时候，与两个随机变量 \(\boldsymbol{x}_{t-1}\) 和 \(\boldsymbol{x}_{t+1}\) 都有关系，因此估计的方差会比单个变量估计的方差大。在“一致性项”中，模型试图让“从 \(t+1\) 推回 \(t\) 的分布”去匹配“从 \(t-1\) 推到 \(t\) 的分布”。因为 \(t+1\) 和 \(t-1\) 都是采样出来的随机样本，它们都在“抖动”，所以计算出来的 Loss 波动会非常大。 随着总扩散步 \(T\) 的增大，一致项估计方差也会线性增长。  因此，这一种的拆解方法并不实用。我们希望最后推导出来的公式，每一项都只与一个随机变量有关。 $$ \[\begin{align} \log p(\boldsymbol{x})...</div></div></div></a><a class="pagination-related" href="/2026/01/23/projects/minimind/minimind-Ch06-Model-%E7%BB%84%E8%A3%85/" title="minimind-Ch06 Model 组装"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">minimind-Ch06 Model 组装</div></div><div class="info-2"><div class="info-item-1">       [{"url":"/img/minimind/architecture.png","alt":"model architecture"}]      在上一章节中，我们又实现了单个的 Transformer 块，在这一章节中，我们将会组装实现上图最左边的整个 Model 结构。 Tokenizer Tokenizer（分词器） 是所有现代自然语言处理（NLP）模型（如 ChatGPT、Llama、BERT 等）的第一道大门。 计算机根本看不懂“苹果”或“Apple”这些字，它只认识数字。Tokenizer 的作用就是把一段文字切分成一个个小块（Token），然后把这些小块转换成计算机能识别的数字 ID。 📒Tokenizer 是人类语言（文本）和机器语言（数字）之间的“翻译官”。  为什么需要 Tokenizer 想象你要教一个只懂数学的机器人学英语：你不能直接给它看句子 “I love AI”。你需要把句子拆开，变成列表：[“I”, “love”, “AI”]。你需要一本字典，规定：“I” = 1, “love” = 2, “AI”...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2026/02/10/notes/diffusion/diffusion-Ch02/" title="diffusion Ch02"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-02-10</div><div class="info-item-2">diffusion Ch02</div></div><div class="info-2"><div class="info-item-1">在上一章中，我们讲了 VDM 的第一种拆解方式，我们最终把 \(\text{ELBO}\) 拆解成了 3 项，分别是重建项、先验匹配项和一致项。这一种拆解的特点可总结如下：  三项都有期望，因此可以用蒙特卡洛方法来进行估计； “一致项”在时间步 \(t\) 的时候，与两个随机变量 \(\boldsymbol{x}_{t-1}\) 和 \(\boldsymbol{x}_{t+1}\) 都有关系，因此估计的方差会比单个变量估计的方差大。在“一致性项”中，模型试图让“从 \(t+1\) 推回 \(t\) 的分布”去匹配“从 \(t-1\) 推到 \(t\) 的分布”。因为 \(t+1\) 和 \(t-1\) 都是采样出来的随机样本，它们都在“抖动”，所以计算出来的 Loss 波动会非常大。 随着总扩散步 \(T\) 的增大，一致项估计方差也会线性增长。  因此，这一种的拆解方法并不实用。我们希望最后推导出来的公式，每一项都只与一个随机变量有关。 $$ \[\begin{align} \log p(\boldsymbol{x})...</div></div></div></a><a class="pagination-related" href="/2026/02/15/notes/diffusion/diffusion-Ch04/" title="diffusion Ch04"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-02-15</div><div class="info-item-2">diffusion Ch04</div></div><div class="info-2"><div class="info-item-1">扩散模型中 reverse-time SDE 推导 待补充，参考  两类扩散模型了解 朗之万动力学去噪分数匹配（SMLD） 我们设定扰动核 \(p_{\sigma}(\tilde{x}|x) := \mathcal{N}(\tilde{x}; x, \sigma^2\mathbf{I})\)，以及扰动（加噪）后的数据边缘概率分布为 \(p_\sigma(\tilde{x}) := \int p_\sigma(\tilde{x}|x) p_\text{data}(x)\text{d}x\)。 考虑一个噪声序列 \(\sigma_\text{min} = \sigma_1 &lt; \sigma_2 &lt; \dots &lt; \sigma_N = \sigma_\text{max}\)，这也就是说每一步的加噪会越来越大。其中，\(\sigma_\text{min}\) 足够小，能够使得 \(p_{\sigma_\text{min}} \approx p_\text{data}(x)\)，\(\sigma_\text{max}\) 足够大，能够使得...</div></div></div></a><a class="pagination-related" href="/2026/02/11/notes/diffusion/diffusion-Ch03/" title="diffusion Ch03"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-02-11</div><div class="info-item-2">diffusion Ch03</div></div><div class="info-2"><div class="info-item-1">条件扩散模型 至此，我们研究的都只是简单的分布 \(p(\boldsymbol{x})\)，然而，在实际应用中，我们经常会对形如 \(p(\boldsymbol{x} | y)\) 这种有条件的分布更感兴趣。这样一来，我们就可以通过条件 \(y\) 来对生成的数据 \(\boldsymbol{x}\) 进行控制。例如，在 image-text 生成模型中，\(y\) 就可以被视作对于输入文本 text 的编码。 回忆我们之前的联合概率密度公式 \[ \begin{equation} p(\boldsymbol{x}_{0:T}) = p(\boldsymbol{x}_T) \prod_{t=1}^{T} p_{\theta}(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t) \label{eq:dist_uncond} \end{equation} \] 我们在分布 \(p(\cdot)\) 中注入条件 \(y\)，即 \(p(\cdot | y)\)，公式 \(\ref{eq:dist_uncond}\) 就会转变成...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.gif" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">creat</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">37</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ardbr"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/Ardbr" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">希望每天都能输出一点新知识😌</div></div><div class="card-widget card-countdown"><div class="countdown-mask"><div class="text-container"><p>2026 年已经过去</p><span class="countdown-number">xx</span><span class="countdown-unit">天</span></div><div class="current-time-container"> <span class="current-time"></span></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="toc-number">1.</span> <span class="toc-text">生成模型的定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%81%E6%8D%AE%E4%B8%8B%E7%95%8Cevidence-lower-bound"><span class="toc-number">2.</span> <span class="toc-text">证据下界（Evidence Lower
Bound）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#vaevariational-autoencoder"><span class="toc-number">3.</span> <span class="toc-text">VAE（Variational Autoencoder）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-autoencoder"><span class="toc-number">3.1.</span> <span class="toc-text">自编码器 (Autoencoder)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%98%E5%88%86-variational"><span class="toc-number">3.2.</span> <span class="toc-text">变分 (Variational)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95"><span class="toc-number">3.3.</span> <span class="toc-text">蒙特卡洛方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%9A%8F%E6%9C%BA%E9%87%87%E6%A0%B7%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">3.3.1.</span> <span class="toc-text">蒙特卡洛随机采样的问题</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E9%87%8D%E6%95%B4%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-number">3.4.</span> <span class="toc-text">参数重整化方法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#hvaehierarchical-variational-autoencoder"><span class="toc-number">4.</span> <span class="toc-text">HVAE（Hierarchical
Variational Autoencoder）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87"><span class="toc-number">4.1.</span> <span class="toc-text">优化目标</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#vdmvariational-diffusion-model"><span class="toc-number">5.</span> <span class="toc-text">VDM（Variational Diffusion
Model）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%99%90%E5%88%B6%E4%B8%80%E7%9A%84%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1"><span class="toc-number">5.1.</span> <span class="toc-text">限制一的数学建模</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%99%90%E5%88%B6%E4%BA%8C%E7%9A%84%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1"><span class="toc-number">5.2.</span> <span class="toc-text">限制二的数学建模</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%99%90%E5%88%B6%E4%B8%89%E7%9A%84%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1"><span class="toc-number">5.3.</span> <span class="toc-text">限制三的数学建模</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E4%BD%93-pipeline"><span class="toc-number">5.4.</span> <span class="toc-text">总体 pipeline</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#vdm-%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87"><span class="toc-number">5.5.</span> <span class="toc-text">VDM 优化目标</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/22/notes/reinforcement_learning/reinforcement-learning-Ch01/" title="reinforcement_learning Ch01">reinforcement_learning Ch01</a><time datetime="2026-02-22T09:17:33.552Z" title="更新于 2026-02-22 17:17:33">2026-02-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/14/notes/flow_matching/flow-matching-Ch02/" title="flow_matching Ch02">flow_matching Ch02</a><time datetime="2026-02-22T09:09:27.296Z" title="更新于 2026-02-22 17:09:27">2026-02-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/15/notes/diffusion/diffusion-Ch04/" title="diffusion Ch04">diffusion Ch04</a><time datetime="2026-02-15T15:08:41.405Z" title="更新于 2026-02-15 23:08:41">2026-02-15</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/12/notes/flow_matching/flow-matching-Ch01/" title="flow_matching Ch01">flow_matching Ch01</a><time datetime="2026-02-15T02:43:06.395Z" title="更新于 2026-02-15 10:43:06">2026-02-15</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/11/notes/diffusion/diffusion-Ch03/" title="diffusion Ch03">diffusion Ch03</a><time datetime="2026-02-15T02:42:57.148Z" title="更新于 2026-02-15 10:42:57">2026-02-15</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2024 - 2026 By creat</div><div class="footer_custom_text">很高兴你能看到这里！</div></div></footer></div><div id="leftside-fps"><span id="leftside-fps-text"></span></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'ams',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }
      
      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script src=/js/sakuraPlus.js></script><script src=/js/parseLyric.js></script><script src=/js/music.js></script><script src=/js/getFps.js></script><script src=/js/countdown.js></script><script src=/js/mathjax.js></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>(() => {
  const pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

  window.pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: pjaxSelectors,
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
  })

  const triggerPjaxFn = (val) => {
    if (!val) return
    Object.values(val).forEach(fn => fn())
  }

  document.addEventListener('pjax:send', () => {
    // removeEventListener
    btf.removeGlobalFnEvent('pjaxSendOnce')
    btf.removeGlobalFnEvent('themeChange')

    // reset readmode
    const $bodyClassList = document.body.classList
    if ($bodyClassList.contains('read-mode')) $bodyClassList.remove('read-mode')

    triggerPjaxFn(window.globalFn.pjaxSend)
  })

  document.addEventListener('pjax:complete', () => {
    btf.removeGlobalFnEvent('pjaxCompleteOnce')
    document.querySelectorAll('script[data-pjax]').forEach(item => {
      const newScript = document.createElement('script')
      const content = item.text || item.textContent || item.innerHTML || ""
      Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
      newScript.appendChild(document.createTextNode(content))
      item.parentNode.replaceChild(newScript, item)
    })

    triggerPjaxFn(window.globalFn.pjaxComplete)
  })

  document.addEventListener('pjax:error', e => {
    if (e.request.status === 404) {
      pjax.loadUrl('/404.html')
    }
  })
})()</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>